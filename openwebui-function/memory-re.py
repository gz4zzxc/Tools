"""
title: è¶…çº§è®°å¿†åŠ©æ‰‹ (Pro Max)
description: v7.6 - åŒ…å«å†å²æ¸…æ´—ä¸ä¸Šä¸‹æ–‡æ„ŸçŸ¥ã€‚æ ¹æ® Code Review è¿›è¡Œäº†æ·±åº¦å·¥ç¨‹åŒ–é‡æ„ï¼Œä¼˜åŒ–äº†ä»£ç å¯è¯»æ€§ã€ç±»å‹å®‰å…¨åŠè¾¹ç•Œæ¡ä»¶å¤„ç†ã€‚
author: å—é£ (äºŒæ”¹Bryce) & Gemini
version: 7.6
required_open_webui_version: >= 0.5.0
"""

import json
import asyncio
import time
import datetime
from typing import Optional, Any, List, Dict, Tuple

import pytz
from pydantic import BaseModel, Field
from fastapi.requests import Request

from open_webui.models.users import Users
from open_webui.routers.memories import (
    add_memory,
    AddMemoryForm,
    query_memory,
    QueryMemoryForm,
    delete_memory_by_id,
)
from open_webui.main import app as webui_app

# ==================== æç¤ºè¯å¸¸é‡ ====================

FACT_EXTRACTION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ã€ç”¨æˆ·ç”»åƒä¾§å†™å¸ˆã€‘ã€‚ä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯ä»å¯¹è¯ä¸­æå–å…³äºç”¨æˆ·çš„**é•¿æœŸäº‹å®**ã€‚

ã€è¾“å…¥æ ¼å¼ã€‘
1. [Context] AI: AI åˆšæ‰è¯´çš„è¯
2. [Target] User: ç”¨æˆ·çš„æœ€æ–°å‘è¨€

ã€æå–é€»è¾‘ã€‘
1. **å¦‚æœæ˜¯ æé—®/æŸ¥è¯¢/æŒ‡ä»¤** (å¦‚ "NVIDIAè‚¡ä»·?", "åˆ†æè´¢æŠ¥") -> ğŸ›‘ **å¿½ç•¥**ã€‚
2. **å¦‚æœæ˜¯ é—²èŠ** -> ğŸ›‘ **å¿½ç•¥**ã€‚
3. **å¦‚æœæ˜¯ è‡ªæˆ‘æŠ«éœ²/é™ˆè¿°äº‹å®** (å¦‚ "æˆ‘æ˜¯è®¾è®¡å¸ˆ", "æˆ‘ä¸åƒè¾£") -> âœ… **è®°å½•**ã€‚
4. **å¦‚æœæ˜¯ å›ç­”/ç¡®è®¤** -> âœ… **ç»“åˆ Context è®°å½•**ã€‚

ã€è¾“å‡ºæ ¼å¼ã€‘
åªè¿”å› JSON å­—ç¬¦ä¸²æ•°ç»„ã€‚æ— äº‹å®è¿”å› `[]`ã€‚
"""

MEMORY_CLEANUP_PROMPT = """ä½ æ˜¯ä¸€åã€è®°å¿†æ•°æ®åº“å®¡è®¡å‘˜ã€‘ã€‚ä½ å°†æ”¶åˆ°ä¸€ä»½ç”¨æˆ·çš„è®°å¿†åˆ—è¡¨ã€‚
å…¶ä¸­åŒ…å«äº†å¾ˆå¤š**é”™è¯¯çš„åƒåœ¾æ•°æ®**ã€‚

ã€åˆ é™¤æ ‡å‡† - é‡åˆ°ä»¥ä¸‹æƒ…å†µå¿…é¡»åˆ é™¤ã€‘
1. ğŸ—‘ï¸ **ä¼ªè£…æˆå…´è¶£çš„æé—®**ï¼š"ç”¨æˆ·å…³æ³¨..." (å…¶å®åªæ˜¯é—®äº†ä¸€å¥)
2. ğŸ—‘ï¸ **å½’å› é”™è¯¯**ï¼š"ç”¨æˆ·åˆ†æäº†..." (å…¶å®æ˜¯AIåˆ†æçš„)
3. ğŸ—‘ï¸ **ä¸´æ—¶ä¿¡æ¯**ï¼š"ç”¨æˆ·è¯¢é—®..."

ã€ä¿ç•™æ ‡å‡†ã€‘
âœ… ç”¨æˆ·å±æ€§ã€æ˜ç¡®åå¥½ã€é•¿æœŸå·¥å…·

ã€è¾“å‡ºæ ¼å¼ã€‘
åªè¿”å›ä¸€ä¸ªåŒ…å«è¦åˆ é™¤ ID çš„ JSON å­—ç¬¦ä¸²æ•°ç»„ã€‚ä¾‹å¦‚ï¼š["id_1", "id_3"]ã€‚å¦‚æœæ²¡æœ‰è¦åˆ é™¤çš„ï¼Œè¿”å› []ã€‚
"""

class Filter:
    # ç±»å˜é‡
    _user_memory_counters: Dict[str, int] = {}
    _summarization_running: set = set()

    class Valves(BaseModel):
        enabled: bool = Field(
            default=True, 
            description="å¼€å¯æˆ–å…³é—­æ’ä»¶åŠŸèƒ½",
            json_schema_extra={"title": "ğŸ”Œ å¯ç”¨æ’ä»¶"}
        )
        # ==================== æ¸…æ´—å¼€å…³ ====================
        enable_retroactive_cleanup: bool = Field(
            default=False, 
            description="å¼€å¯åï¼Œæ¯æ¬¡å¯¹è¯è§¦å‘åå°ä»»åŠ¡ï¼Œæ‰«æå¹¶åˆ é™¤é”™è¯¯çš„è®°å¿†ã€‚æ¸…æ´—å®Œæˆåè¯·åŠ¡å¿…å…³é—­ï¼",
            json_schema_extra={"title": "ğŸ§¹ å¼€å¯å†å²æ¸…æ´—æ¨¡å¼ (ç”¨å®Œå³å…³)"}
        )
        cleanup_batch_size: int = Field(
            default=50,
            description="æ¯æ¬¡æ¸…æ´—æ‰«æçš„è®°å¿†æ¡æ•° (å»ºè®® 50-100)",
            json_schema_extra={"title": "ğŸ§¹ å•æ¬¡æ‰«ææ•°é‡"}
        )
        # ====================================================
        api_url: str = Field(
            default="https://api.openai.com/v1/chat/completions", 
            description="LLM API åœ°å€",
            json_schema_extra={"title": "ğŸ¤– API åœ°å€"}
        )
        api_key: str = Field(
            default="", 
            description="LLM API Key",
            json_schema_extra={"title": "ğŸ”‘ API Key"}
        )
        model: str = Field(
            default="gpt-4o-mini", 
            description="æ¨¡å‹",
            json_schema_extra={"title": "ğŸ§  å¤„ç†æ¨¡å‹"}
        )
        show_stats: bool = Field(
            default=True, 
            description="æ˜¾ç¤ºç»Ÿè®¡",
            json_schema_extra={"title": "ğŸ“Š æ˜¾ç¤ºç»Ÿè®¡"}
        )
        messages_to_consider: int = Field(
            default=2, 
            description="ä¸Šä¸‹æ–‡çª—å£",
            json_schema_extra={"title": "ğŸ” åˆ†æçª—å£"}
        )
        timezone: str = Field(
            default="Asia/Shanghai", 
            description="æ—¶åŒº",
            json_schema_extra={"title": "ğŸŒ æ—¶åŒº"}
        )
        consolidation_threshold: float = Field(
            default=0.75, 
            description="ç›¸ä¼¼åº¦é˜ˆå€¼",
            json_schema_extra={"title": "ğŸ”— ç›¸ä¼¼åº¦é˜ˆå€¼"}
        )
        summarize_after_n_memories: int = Field(
            default=10, 
            description="æ•´ç†é¢‘ç‡",
            json_schema_extra={"title": "ğŸ“¦ æ•´ç†é¢‘ç‡"}
        )

    def __init__(self):
        self.valves = self.Valves()
        self.start_time: float = 0.0
        self.time_to_first_token: Optional[float] = None
        self.first_chunk_received: bool = False

    def inlet(self, body: dict, __user__: Optional[dict] = None) -> dict:
        self.start_time = time.time()
        self.time_to_first_token = None
        self.first_chunk_received = False
        return body

    def stream(self, event: dict) -> dict:
        if not self.first_chunk_received:
            self.time_to_first_token = time.time() - self.start_time
            self.first_chunk_received = True
        return event

    async def outlet(self, body: dict, __event_emitter__: Any, __user__: Optional[dict] = None) -> dict:
        """ä¸»è¾“å‡ºå¤„ç†é€»è¾‘"""
        if not self.valves.enabled or not __user__ or len(body.get("messages", [])) < 2:
            return body

        user = Users.get_user_by_id(__user__["id"])
        conversation_end_time = time.time()

        # åˆå§‹åŒ–ç»“æœå¯¹è±¡
        memory_result: Dict[str, Any] = {"status": "skipped", "message": ""}
        
        # åˆ†æ”¯é€»è¾‘ï¼šæ¸…æ´—æ¨¡å¼ vs æ­£å¸¸æ¨¡å¼
        if self.valves.enable_retroactive_cleanup:
            # å¯åŠ¨æ¸…æ´—ä»»åŠ¡
            asyncio.create_task(self._run_retroactive_cleanup(user))
            memory_result = {
                "status": "success",
                "message": "ğŸ§¹ å†å²æ¸…æ´—å·²å¯åŠ¨"
            }
        else:
            # æ­£å¸¸è®°å¿†å¤„ç†
            try:
                memory_result = await self._process_memory(body, user)
            except Exception as e:
                print(f"[SuperMemory] Processing Error: {e}")
                memory_result = {
                    "status": "error",
                    "message": "âš ï¸ å¤„ç†å¼‚å¸¸"
                }

        # æ˜¾ç¤ºçŠ¶æ€æ 
        if self.valves.show_stats:
            stats = self._calculate_stats(conversation_end_time)
            await self._show_status(__event_emitter__, memory_result, stats)

        return body

    # ==================== ğŸ§¹ å†å²æ¸…æ´—é€»è¾‘ ====================

    async def _run_retroactive_cleanup(self, user: Any) -> None:
        """åå°ä»»åŠ¡ï¼šæ‹‰å–æ—§è®°å¿† -> LLM å®¡è®¡ -> åˆ é™¤åƒåœ¾"""
        # 1. å‚æ•°é˜²å¾¡æ€§æ£€æŸ¥
        batch_size = self.valves.cleanup_batch_size
        if batch_size <= 0:
            print("[Cleaner] Batch size invalid, skip.")
            return

        print(f"[Cleaner] Starting cleanup task for user {user.id} (Batch: {batch_size})...")
        
        req = Request(scope={"type": "http", "app": webui_app})
        try:
            # ä½¿ç”¨ç©ºæ ¼ä½œä¸ºé€šé…ç¬¦æŸ¥è¯¢ï¼Œè¿™æ˜¯ Vector DB çš„å¸¸è§ Trick
            result = await query_memory(req, QueryMemoryForm(content=" ", k=batch_size), user)
            
            # æ£€æŸ¥æŸ¥è¯¢ç»“æœæœ‰æ•ˆæ€§
            if not (result and hasattr(result, 'ids') and result.ids and result.ids[0]):
                print("[Cleaner] No memories found to clean.")
                return

            ids = result.ids[0]
            docs = result.documents[0]
            
            # 2. æ„å»ºå®¡è®¡æ•°æ®
            memory_list_str = ""
            valid_batch_ids = []
            
            for i, content in enumerate(docs):
                mem_id = ids[i]
                valid_batch_ids.append(mem_id)
                memory_list_str += f"ID: {mem_id} | Content: {content}\n"

            if not memory_list_str:
                return

            # 3. LLM å®¡è®¡
            print(f"[Cleaner] Auditing {len(valid_batch_ids)} memories...")
            ids_to_delete = await self._call_llm_json(MEMORY_CLEANUP_PROMPT, memory_list_str)
            
            if not ids_to_delete:
                print("[Cleaner] Audit passed. No garbage found.")
                return

            # 4. æ‰§è¡Œåˆ é™¤
            deleted_count = 0
            for mid in ids_to_delete:
                if mid in valid_batch_ids:
                    try:
                        await delete_memory_by_id(mid, user)
                        deleted_count += 1
                        print(f"[Cleaner] Deleted garbage: {mid}")
                    except Exception as e:
                        print(f"[Cleaner] Delete failed {mid}: {e}")
            
            print(f"[Cleaner] Cleanup complete. Deleted {deleted_count} items.")

        except Exception as e:
            print(f"[Cleaner] Critical Error: {e}")

    # ==================== æ­£å¸¸è®°å¿†æµç¨‹ ====================

    async def _process_memory(self, body: dict, user: Any) -> Dict[str, Any]:
        """å¤„ç†æ–°å¯¹è¯ï¼Œæå–äº‹å®å¹¶å­˜å‚¨"""
        # 1. æ„å»ºä¸Šä¸‹æ–‡
        context_str = self._build_context_string(body["messages"])
        if not context_str:
            return {"status": "skipped", "message": "ğŸ” æ— æœ‰æ•ˆä¸Šä¸‹æ–‡"}

        # 2. æå–äº‹å®
        new_facts = await self._call_llm_json(FACT_EXTRACTION_PROMPT, context_str)
        if not new_facts:
            return {"status": "success", "message": "ğŸ’¨ æ— æ–°äº‹å®"}

        saved_count = 0
        updated_count = 0
        
        # 3. é€æ¡å¤„ç†äº‹å®
        for fact in new_facts:
            if not isinstance(fact, str):
                continue 
            
            # æŸ¥é‡
            similar_memories = await self._query_similar_memories(fact, user)
            
            # å…³ç³»åˆ¤æ–­
            action, target_ids = await self._analyze_relationship(fact, similar_memories)
            
            if action == "skip":
                continue
            
            # æ‰§è¡Œå­˜å‚¨/æ›´æ–°
            try:
                if action == "update" and target_ids:
                    for mid in target_ids:
                        await delete_memory_by_id(mid, user)
                    updated_count += 1
                else:
                    saved_count += 1
                
                await self._save_memory_native(fact, user)
            except Exception as e:
                print(f"[SuperMemory] Save Error: {e}")
                continue
            
            # è§¦å‘æ‘˜è¦è®¡æ•°
            self._increment_counter_and_trigger_summary(user)

        # æ„å»ºè¿”å›ä¿¡æ¯ï¼ˆå¸¦ emoji ç¾åŒ–ï¼‰
        msg_parts = []
        if saved_count:
            msg_parts.append(f"âœ¨ æ–°å¢ {saved_count}")
        if updated_count:
            msg_parts.append(f"ğŸ”„ æ›´æ–° {updated_count}")

        final_message = " Â· ".join(msg_parts) if msg_parts else "ğŸ’­ æ— éœ€è®°å¿†"
        return {"status": "success", "message": final_message}

    # ==================== è¾…åŠ©æ–¹æ³• ====================

    def _build_context_string(self, messages: List[dict]) -> str:
        """æ„å»º [AI] -> [User] çš„ä¸Šä¸‹æ–‡å¯¹ï¼Œç”¨äºå‡†ç¡®çš„æ„å›¾è¯†åˆ«"""
        if not messages:
            return ""
            
        last_user_idx = -1
        # å€’åºæŸ¥æ‰¾æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        for i in range(len(messages) - 1, -1, -1):
            if messages[i]['role'] == 'user':
                last_user_idx = i
                break
                
        if last_user_idx == -1:
            return ""
            
        target_user_msg = messages[last_user_idx]['content']
        context_ai_msg = "æ— "
        
        # è·å–è¯¥ç”¨æˆ·æ¶ˆæ¯çš„å‰ä¸€æ¡ AI æ¶ˆæ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if last_user_idx > 0 and messages[last_user_idx - 1]['role'] == 'assistant':
            context_ai_msg = messages[last_user_idx - 1]['content']
            
        return f"[Context] AI: {context_ai_msg}\n[Target] User: {target_user_msg}"

    async def _save_memory_native(self, content: str, user: Any) -> None:
        """è°ƒç”¨ç³»ç»Ÿ API å­˜å‚¨è®°å¿†ï¼Œå¸¦æ—¶é—´æˆ³"""
        try:
            tz = pytz.timezone(self.valves.timezone)
        except pytz.UnknownTimeZoneError:
            tz = pytz.utc
            
        now_str = datetime.datetime.now(tz).strftime('%Yå¹´%mæœˆ%dæ—¥%Hç‚¹%Måˆ†')
        final_content = f"{now_str}ï¼š{content}"
        
        req = Request(scope={"type": "http", "app": webui_app})
        await add_memory(req, AddMemoryForm(content=final_content), user)

    async def _query_similar_memories(self, content: str, user: Any) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢ç›¸ä¼¼è®°å¿†"""
        req = Request(scope={"type": "http", "app": webui_app})
        try:
            result = await query_memory(req, QueryMemoryForm(content=content, k=5), user)
            memories = []
            if result and hasattr(result, 'ids') and result.ids:
                ids = result.ids[0]
                docs = result.documents[0]
                dists = result.distances[0]
                for i, doc in enumerate(docs):
                    similarity = 1 - dists[i]
                    if similarity >= self.valves.consolidation_threshold:
                        memories.append({
                            "id": ids[i], 
                            "content": doc, 
                            "similarity": similarity
                        })
            return memories
        except Exception:
            return []

    async def _analyze_relationship(self, new_fact: str, similar_memories: List[dict]) -> Tuple[str, List[str]]:
        """åˆ†ææ–°æ—§è®°å¿†å…³ç³»ï¼šduplicate / update / new"""
        if not similar_memories:
            return "new", []
            
        context_list = [m['content'] for m in similar_memories]
        prompt = (
            f"æ–°ä¿¡æ¯: {new_fact}\n\nç›¸å…³æ—§è®°å¿†:\n" 
            + "\n".join(context_list) 
            + "\n\nè¯·åˆ¤æ–­å…³ç³»ï¼Œåªè¿”å›å•è¯: duplicate (é‡å¤), update (éœ€æ›´æ–°æ—§è®°å¿†), new (æ–°ä¿¡æ¯)"
        )
        
        try:
            res = await self._call_llm(prompt, system_prompt="ä½ æ˜¯ä¸€ä¸ªå»é‡åˆ¤æ–­å™¨ã€‚")
            res = res.lower().strip()
            
            # ä»£ç è§£å‹ï¼Œæé«˜å¯è¯»æ€§
            if "duplicate" in res:
                return "skip", []
            elif "update" in res:
                return "update", [m['id'] for m in similar_memories]
            else:
                return "new", []
        except Exception:
            # å‡ºé”™æ—¶é»˜è®¤ä½œä¸ºæ–°è®°å¿†å­˜å‚¨ï¼Œé¿å…ä¸¢å¤±ä¿¡æ¯
            return "new", []

    def _increment_counter_and_trigger_summary(self, user: Any) -> None:
        """ç®€å•çš„æ‘˜è¦è§¦å‘è®¡æ•°å™¨"""
        uid = user.id
        count = self._user_memory_counters.get(uid, 0) + 1
        self._user_memory_counters[uid] = count
        
        if count >= self.valves.summarize_after_n_memories:
            if uid not in self._summarization_running:
                self._user_memory_counters[uid] = 0
                asyncio.create_task(self._run_consolidation_task(user))

    async def _run_consolidation_task(self, user: Any) -> None:
        """åå°æ‘˜è¦ä»»åŠ¡å ä½ç¬¦"""
        uid = user.id
        self._summarization_running.add(uid)
        try:
            await asyncio.sleep(0.1)
        except Exception:
            pass
        finally:
            self._summarization_running.discard(uid)

    async def _call_llm(self, prompt: str, system_prompt: str = "") -> str:
        import aiohttp
        headers = {
            "Authorization": f"Bearer {self.valves.api_key}", 
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.valves.model, 
            "messages": [
                {"role": "system", "content": system_prompt}, 
                {"role": "user", "content": prompt}
            ], 
            "temperature": 0.0
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(self.valves.api_url, headers=headers, json=payload) as resp:
                if resp.status != 200:
                    raise Exception(f"API Error: {resp.status}")
                data = await resp.json()
                return data["choices"][0]["message"]["content"].strip()

    async def _call_llm_json(self, system_prompt: str, user_prompt: str) -> List[str]:
        try:
            text = await self._call_llm(user_prompt, system_prompt)
            # å…¼å®¹ Markdown ä»£ç å—æ ¼å¼
            if "```json" in text:
                text = text.split("```json")[1].split("```")[0]
            elif "```" in text:
                text = text.split("```")[1].split("```")[0]
                
            result = json.loads(text)
            return result if isinstance(result, list) else []
        except Exception:
            return []

    def _calculate_stats(self, end_time: float) -> Dict[str, str]:
        elapsed = end_time - self.start_time
        ttft = "N/A"
        if self.time_to_first_token is not None:
            ttft = f"{self.time_to_first_token:.2f}s"
        return {"elapsed": f"{elapsed:.2f}s", "ttft": ttft}

    async def _show_status(self, emitter: Any, memory_res: Dict[str, Any], stats: Dict[str, str]) -> None:
        """åœ¨ UI ä¸Šæ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯ï¼ˆå¸¦ emoji ç¾åŒ–ï¼‰"""
        # æ ¹æ®çŠ¶æ€é€‰æ‹©ä¸åŒçš„ emoji
        status_emoji = {
            "success": "ğŸ§ ",
            "error": "âŒ",
            "skipped": "â­ï¸",
        }.get(memory_res.get("status", "skipped"), "ğŸ“")

        # æ„å»ºç¾è§‚çš„çŠ¶æ€æ 
        status_text = (
            f"{status_emoji} {memory_res.get('message', '')}  "
            f"âš¡ {stats['ttft']}  "
            f"â±ï¸ {stats['elapsed']}"
        )
        await emitter({
            "type": "status",
            "data": {"description": status_text, "done": True}
        })