"""
title: è¶…çº§è®°å¿†åŠ©æ‰‹ (Pro)
description: è‡ªåŠ¨æå–å¯¹è¯äº‹å®ã€å»é‡ã€å¹¶æ·»åŠ ç²¾ç¡®æ—¶é—´æˆ³ã€‚ä¼˜åŒ–äº†é…ç½®ç•Œé¢å’Œç±»å‹å®‰å…¨ã€‚
author: å—é£ (äºŒæ”¹Bryce) & Gemini
version: 7.2
required_open_webui_version: >= 0.5.0
"""

import json
import asyncio
import time
import datetime
import re
from typing import Optional, Callable, Awaitable, Any, List, Dict, Tuple

import pytz
from pydantic import BaseModel, Field
from fastapi.requests import Request

# æ ¸å¿ƒå¼•ç”¨ï¼šä¿æŒä½¿ç”¨ Routerï¼Œè¿™æ˜¯ç›®å‰å¤ç”¨ç³»ç»Ÿ Embedding æœ€ç¨³å®šçš„æ–¹å¼
from open_webui.models.users import Users
from open_webui.routers.memories import (
    add_memory,
    AddMemoryForm,
    query_memory,
    QueryMemoryForm,
    delete_memory_by_id,
)
from open_webui.main import app as webui_app

# ==================== æç¤ºè¯å¸¸é‡ ====================
FACT_EXTRACTION_PROMPT = """ä½ æ­£åœ¨å¸®åŠ©ç»´æŠ¤ç”¨æˆ·çš„â€œè®°å¿†â€ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ¤æ–­ç”¨æˆ·çš„ã€æœ€æ–°ä¸€æ¡ã€‘æ¶ˆæ¯ä¸­ï¼Œæœ‰å“ªäº›ç»†èŠ‚å€¼å¾—ä½œä¸ºâ€œè®°å¿†â€è¢«é•¿æœŸä¿å­˜ã€‚\nã€æ ¸å¿ƒæŒ‡ä»¤ã€‘\n1. åªåˆ†æç”¨æˆ·æœ€æ–°ä¸€æ¡æ¶ˆæ¯ã€‚\n2. å¿½ç•¥ä¸´æ—¶ä¿¡æ¯ã€‚\n3. è¿”å›JSONå­—ç¬¦ä¸²æ•°ç»„ï¼Œå¦‚ ["ç”¨æˆ·å–œæ¬¢åƒè‹¹æœ", "ç”¨æˆ·æ˜¯ä¸€åç¨‹åºå‘˜"]ã€‚"""

class Filter:
    # ç±»å˜é‡ï¼šç”¨äºç®€å•çš„è·¨è¯·æ±‚çŠ¶æ€è·Ÿè¸ª
    _user_memory_counters: Dict[str, int] = {}
    _summarization_running: set = set()

    class Valves(BaseModel):
        """
        é…ç½®é¡¹ç±» - ä½¿ç”¨ title å±æ€§ä¼˜åŒ–å‰ç«¯æ˜¾ç¤º
        """
        enabled: bool = Field(
            default=True, 
            description="å¼€å¯æˆ–å…³é—­æ’ä»¶åŠŸèƒ½",
            json_schema_extra={"title": "ğŸ”Œ å¯ç”¨æ’ä»¶"}
        )
        api_url: str = Field(
            default="https://api.openai.com/v1/chat/completions", 
            description="ç”¨äºæå–è®°å¿†çš„ LLM API åœ°å€",
            json_schema_extra={"title": "ğŸ¤– API åœ°å€"}
        )
        api_key: str = Field(
            default="", 
            description="ç”¨äºæå–è®°å¿†çš„ API Key",
            json_schema_extra={"title": "ğŸ”‘ API Key"}
        )
        model: str = Field(
            default="gpt-4o-mini", 
            description="å»ºè®®ä½¿ç”¨å¿«é€Ÿä¸”æ™ºèƒ½çš„æ¨¡å‹",
            json_schema_extra={"title": "ğŸ§  å¤„ç†æ¨¡å‹"}
        )
        show_stats: bool = Field(
            default=True, 
            description="åœ¨å¯¹è¯ç»“æŸåæ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡",
            json_schema_extra={"title": "ğŸ“Š æ˜¾ç¤ºç»Ÿè®¡"}
        )
        messages_to_consider: int = Field(
            default=6, 
            description="æå–äº‹å®æ—¶å‚è€ƒçš„æœ€è¿‘æ¶ˆæ¯æ•°é‡",
            json_schema_extra={"title": "ğŸ” ä¸Šä¸‹æ–‡çª—å£"}
        )
        timezone: str = Field(
            default="Asia/Shanghai", 
            description="ç”¨äºç”Ÿæˆè®°å¿†æ—¶é—´æˆ³çš„æ—¶åŒº",
            json_schema_extra={"title": "ğŸŒ æ—¶åŒº"}
        )
        consolidation_threshold: float = Field(
            default=0.75, 
            description="åˆ¤æ–­è®°å¿†ç›¸ä¼¼åº¦çš„é˜ˆå€¼ (0.0-1.0)",
            json_schema_extra={"title": "ğŸ”— ç›¸ä¼¼åº¦é˜ˆå€¼"}
        )
        summarize_after_n_memories: int = Field(
            default=10, 
            description="æ¯æ–°å¢å¤šå°‘æ¡è®°å¿†è§¦å‘ä¸€æ¬¡æ•´ç†",
            json_schema_extra={"title": "ğŸ“¦ æ•´ç†é¢‘ç‡"}
        )

    def __init__(self):
        self.valves = self.Valves()
        self.start_time: float = 0.0
        self.time_to_first_token: Optional[float] = None
        self.first_chunk_received: bool = False

    def inlet(self, body: dict, __user__: Optional[dict] = None) -> dict:
        """è¯·æ±‚é¢„å¤„ç†ï¼šé‡ç½®è®¡æ—¶å™¨"""
        self.start_time = time.time()
        self.time_to_first_token = None
        self.first_chunk_received = False
        return body

    def stream(self, event: dict) -> dict:
        """æµå¼å¤„ç†ï¼šæ•è·é¦–å­—æ—¶é—´"""
        if not self.first_chunk_received:
            self.time_to_first_token = time.time() - self.start_time
            self.first_chunk_received = True
        return event

    async def outlet(self, body: dict, __event_emitter__: Callable[[Any], Awaitable[None]], __user__: Optional[dict] = None) -> dict:
        """å“åº”åå¤„ç†ï¼šæ‰§è¡Œè®°å¿†é€»è¾‘"""
        if not self.valves.enabled or not __user__ or len(body.get("messages", [])) < 2:
            return body

        user = Users.get_user_by_id(__user__["id"])
        conversation_end_time = time.time()

        # æ ¸å¿ƒé€»è¾‘ï¼šæå–å¹¶ä¿å­˜è®°å¿†
        memory_result = {"status": "skipped", "message": ""}
        try:
            # æ‰§è¡Œè®°å¿†å¤„ç†
            memory_result = await self._process_memory(body, user)
        except Exception as e:
            print(f"[SuperMemory] Processing Error: {e}")
            memory_result = {"status": "error", "message": "å¤„ç†å‡ºé”™"}

        # ç»Ÿè®¡ä¿¡æ¯å±•ç¤º
        if self.valves.show_stats:
            stats = self._calculate_stats(conversation_end_time)
            await self._show_status(__event_emitter__, memory_result, stats)

        return body

    # ==================== æ ¸å¿ƒé€»è¾‘ ====================

    async def _process_memory(self, body: dict, user: Any) -> Dict[str, Any]:
        """
        ä¸»æµç¨‹ï¼šæå–äº‹å® -> æŸ¥é‡ -> å­˜å‚¨ -> (å¯é€‰)è§¦å‘æ‘˜è¦
        """
        conversation_text = self._stringify_conversation(body["messages"])
        
        # 1. æå–äº‹å®
        new_facts = await self._call_llm_json(FACT_EXTRACTION_PROMPT, conversation_text)
        if not new_facts:
            return {"status": "success", "message": "æ— æ–°äº‹å®"}

        saved_count = 0
        updated_count = 0
        
        for fact in new_facts:
            if not isinstance(fact, str): continue # ç±»å‹å®‰å…¨æ£€æŸ¥

            # 2. æŸ¥é‡ï¼šæŸ¥æ‰¾ç›¸ä¼¼è®°å¿†
            similar_memories = await self._query_similar_memories(fact, user)
            
            # 3. æ™ºèƒ½åˆ¤æ–­ï¼šå†³å®šæ˜¯è·³è¿‡ã€æ›´æ–°è¿˜æ˜¯æ–°å¢
            action, target_ids = await self._analyze_relationship(fact, similar_memories)
            
            if action == "skip":
                continue
            
            # 4. æ‰§è¡Œæ“ä½œ
            try:
                if action == "update" and target_ids:
                    for mid in target_ids:
                        await delete_memory_by_id(mid, user)
                    updated_count += 1
                else:
                    saved_count += 1
                
                # å­˜å…¥æ–°è®°å¿† (å¸¦æ—¶é—´æˆ³)
                await self._save_memory_native(fact, user)
            except Exception as e:
                print(f"[SuperMemory] Save/Update failed: {e}")
                continue
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘åå°æ•´ç†
            self._increment_counter_and_trigger_summary(user)

        # æ„å»ºè¿”å›æ¶ˆæ¯
        msg_parts = []
        if saved_count: msg_parts.append(f"æ–°å¢{saved_count}")
        if updated_count: msg_parts.append(f"æ›´æ–°{updated_count}")
        
        return {
            "status": "success", 
            "message": ", ".join(msg_parts) if msg_parts else "ä¿¡æ¯å·²å­˜åœ¨",
        }

    async def _save_memory_native(self, content: str, user: Any) -> None:
        """
        æ„å»ºæ—¶é—´æˆ³å¹¶è°ƒç”¨ç³»ç»Ÿ API å­˜å‚¨
        è¯´æ˜ï¼šä½¿ç”¨ Request(scope=...) æ˜¯ä¸ºäº†å…¼å®¹ OpenWebUI çš„ Router å†…éƒ¨è°ƒç”¨æœºåˆ¶
        """
        try:
            tz = pytz.timezone(self.valves.timezone)
        except pytz.UnknownTimeZoneError:
            tz = pytz.utc
            
        now_str = datetime.datetime.now(tz).strftime('%Yå¹´%mæœˆ%dæ—¥%Hç‚¹%Måˆ†')
        final_content = f"{now_str}ï¼š{content}"
        
        # Mock ä¸€ä¸ª Request å¯¹è±¡ï¼Œè¿™æ˜¯è°ƒç”¨ Router çš„å¿…è¦æ¡ä»¶
        req = Request(scope={"type": "http", "app": webui_app})
        await add_memory(req, AddMemoryForm(content=final_content), user)

    async def _query_similar_memories(self, content: str, user: Any) -> List[Dict[str, Any]]:
        """ä½¿ç”¨åŸç”Ÿ API æŸ¥æ‰¾ç›¸ä¼¼è®°å¿†"""
        req = Request(scope={"type": "http", "app": webui_app})
        try:
            result = await query_memory(
                req, 
                QueryMemoryForm(content=content, k=5), 
                user
            )
            memories = []
            if result and hasattr(result, 'ids') and result.ids:
                ids = result.ids[0]
                docs = result.documents[0]
                dists = result.distances[0]
                
                for i, doc in enumerate(docs):
                    similarity = 1 - dists[i]
                    if similarity >= self.valves.consolidation_threshold:
                        memories.append({
                            "id": ids[i],
                            "content": doc,
                            "similarity": similarity
                        })
            return memories
        except Exception as e:
            print(f"[SuperMemory] Query error: {e}")
            return []

    async def _analyze_relationship(self, new_fact: str, similar_memories: List[dict]) -> Tuple[str, List[str]]:
        """åˆ¤æ–­æ–°äº‹å®ä¸æ—§è®°å¿†çš„å…³ç³»"""
        if not similar_memories:
            return "new", []
        
        context_list = [m['content'] for m in similar_memories]
        prompt = (
            f"æ–°ä¿¡æ¯: {new_fact}\n\nç›¸å…³æ—§è®°å¿†:\n" 
            + "\n".join(context_list) 
            + "\n\nè¯·åˆ¤æ–­å…³ç³»ï¼Œåªè¿”å›å•è¯: duplicate (é‡å¤), update (éœ€æ›´æ–°æ—§è®°å¿†), new (æ–°ä¿¡æ¯)"
        )
        
        try:
            res = await self._call_llm(prompt, system_prompt="ä½ æ˜¯ä¸€ä¸ªå»é‡åˆ¤æ–­å™¨ã€‚")
            res = res.lower().strip()
            
            if "duplicate" in res:
                return "skip", []
            elif "update" in res:
                return "update", [m['id'] for m in similar_memories]
            else:
                return "new", []
        except Exception:
            # LLM è°ƒç”¨å¤±è´¥æ—¶ï¼Œé»˜è®¤ä¿å®ˆç­–ç•¥ï¼šå­˜ä¸ºæ–°è®°å¿†
            return "new", []

    # ==================== åå°ä»»åŠ¡ ====================

    def _increment_counter_and_trigger_summary(self, user: Any) -> None:
        uid = user.id
        count = self._user_memory_counters.get(uid, 0) + 1
        self._user_memory_counters[uid] = count
        
        if count >= self.valves.summarize_after_n_memories:
            if uid not in self._summarization_running:
                self._user_memory_counters[uid] = 0
                asyncio.create_task(self._run_consolidation_task(user))

    async def _run_consolidation_task(self, user: Any) -> None:
        uid = user.id
        self._summarization_running.add(uid)
        try:
            # å¯ä»¥åœ¨æ­¤å®ç°æ›´å¤æ‚çš„æ‘˜è¦åˆå¹¶é€»è¾‘
            # ç›®å‰ä»…ä½œä¸ºå ä½ï¼Œé¿å…æŠ¥é”™
            await asyncio.sleep(0.1) 
        except Exception as e:
            print(f"[SuperMemory] Consolidation task failed: {e}")
        finally:
            self._summarization_running.discard(uid)

    # ==================== å·¥å…·å‡½æ•° ====================

    async def _call_llm(self, prompt: str, system_prompt: str = "") -> str:
        import aiohttp
        headers = {
            "Authorization": f"Bearer {self.valves.api_key}", 
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.valves.model,
            "messages": [
                {"role": "system", "content": system_prompt}, 
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.0
        }
        async with aiohttp.ClientSession() as session:
            async with session.post(self.valves.api_url, headers=headers, json=payload) as resp:
                if resp.status != 200:
                    raise Exception(f"API Error: {resp.status}")
                data = await resp.json()
                return data["choices"][0]["message"]["content"].strip()

    async def _call_llm_json(self, system_prompt: str, user_prompt: str) -> List[str]:
        try:
            text = await self._call_llm(user_prompt, system_prompt)
            # æ¸…ç† Markdown ä»£ç å—
            if "```json" in text:
                text = text.split("```json")[1].split("```")[0]
            elif "```" in text:
                text = text.split("```")[1].split("```")[0]
            
            result = json.loads(text)
            return result if isinstance(result, list) else []
        except Exception as e:
            print(f"[SuperMemory] JSON parsing error: {e}")
            return []

    def _stringify_conversation(self, messages: List[dict]) -> str:
        # æ’é™¤ System Messageï¼Œåªçœ‹æœ€è¿‘å‡ è½®
        valid_msgs = [m for m in messages if m['role'] in ('user', 'assistant')]
        return "\n".join([f"{m['role']}: {m['content']}" for m in valid_msgs[-self.valves.messages_to_consider:]])

    def _calculate_stats(self, end_time: float) -> Dict[str, str]:
        elapsed = end_time - self.start_time
        ttft = "N/A"
        if self.time_to_first_token is not None:
            ttft = f"{self.time_to_first_token:.2f}s"
            
        return {
            "elapsed": f"{elapsed:.2f}s",
            "ttft": ttft
        }

    async def _show_status(self, emitter: Callable, memory_res: Dict, stats: Dict) -> None:
        status_text = f"è®°å¿†: {memory_res['message']} | é¦–å­—: {stats['ttft']} | è€—æ—¶: {stats['elapsed']}"
        await emitter({
            "type": "status", 
            "data": {"description": status_text, "done": True}
        })